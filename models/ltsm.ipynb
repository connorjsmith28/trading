{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9fc89d59a83b23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:29.669959300Z",
     "start_time": "2024-08-28T17:02:28.683902900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe54a882ded20973",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:34.772391400Z",
     "start_time": "2024-08-28T17:02:29.670960500Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\connor\\PycharmProjects\\trading\\data\\analytics\\analytics_voo.csv'\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e113a86696fd67",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:34.782173100Z",
     "start_time": "2024-08-28T17:02:34.773391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['date', 'open', 'high', 'low', 'close', 'volume', 'join_date', 'Id',\n       'Date', 'DateLongDescription', 'DateShortDescription', 'DayLongName',\n       'DayShortName', 'MonthLongName', 'MonthShortName', 'CalendarDay',\n       'CalendarWeek', 'CalendarWeekStartDateId', 'CalendarWeekEndDateId',\n       'CalendarDayInWeek', 'CalendarMonth', 'CalendarMonthStartDateId',\n       'CalendarMonthEndDateId', 'CalendarNumberOfDaysInMonth',\n       'CalendarDayInMonth', 'CalendarQuarter', 'CalendarQuarterStartDateId',\n       'CalendarQuarterEndDateId', 'CalendarQuarterStartDate',\n       'CalendarNumberOfDaysInQuarter', 'CalendarDayInQuarter', 'CalendarYear',\n       'CalendarYearEndDateId', 'CalendarYearStartDate',\n       'CalendarNumberOfDaysInYear', 'month_join_key', 'year_join_key',\n       'seven_day_ema', 'CPALTT01USM657N', 'DFF', 'EXPINF10YR', 'GDPC1',\n       'RSXFS', 'T10YFF', 'UNRATE', 'macd', 'macd_signal', 'macd_hist',\n       'daily_obv', 'target'],\n      dtype='object')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c8b93f981cd0f5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:35.443162800Z",
     "start_time": "2024-08-28T17:02:34.779172600Z"
    }
   },
   "outputs": [],
   "source": [
    "non_target_columns = ['open', 'high', 'low', 'close', 'volume', 'CPALTT01USM657N', 'DFF', 'EXPINF10YR', 'GDPC1', 'RSXFS', 'T10YFF', 'UNRATE', 'macd', 'macd_signal', 'macd_hist', 'daily_obv', 'seven_day_ema', 'close', 'target']\n",
    "\n",
    "df_cols = df[non_target_columns]\n",
    "\n",
    "# version with just close price and date\n",
    "df_target_train = df[['date', 'target']].where(df.date <= '2020-01-01')\n",
    "df_target_train.dropna(inplace=True)\n",
    "df_target_test = df[['date', 'target']].where(df.date > '2020-01-01')\n",
    "df_target_test.dropna(inplace=True)\n",
    "\n",
    "# version with all columns\n",
    "df_target_train_v2 = df[non_target_columns].where(df.date <= '2020-01-01')\n",
    "df_target_train_v2.dropna(inplace=True)\n",
    "df_target_test_v2 = df[non_target_columns].where(df.date > '2020-01-01')\n",
    "df_target_test_v2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6c67e1f20487e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:35.445166700Z",
     "start_time": "2024-08-28T17:02:35.443162800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'target'], dtype='object')\n",
      "\n",
      "Index(['open', 'high', 'low', 'close', 'volume', 'CPALTT01USM657N', 'DFF',\n",
      "       'EXPINF10YR', 'GDPC1', 'RSXFS', 'T10YFF', 'UNRATE', 'macd',\n",
      "       'macd_signal', 'macd_hist', 'daily_obv', 'seven_day_ema', 'close',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_target_train.columns)\n",
    "print()\n",
    "print(df_target_train_v2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617b535721c12446",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:35.452166200Z",
     "start_time": "2024-08-28T17:02:35.446241200Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a13bd55cd725fa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:02:35.453167200Z",
     "start_time": "2024-08-28T17:02:35.449258100Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_length, num_rows=len(df), include_all_features=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df: pandas dataframe with at least 2 columns, a date column and a target column\n",
    "    seq_length: How long the sequence should be. \n",
    "    num_rows: How many rows to use. Will pull all rows unless num_rows is provided, in which case the top num_rows will be pulled.\n",
    "    include_all_features: Whether to use just the target column for the sequence, or to include all features.\n",
    "        \n",
    "    Output:\n",
    "    Generates a sequence of seq_length length. It's shape is [num_rows, seq_length] if include_all_features is False, else [num_rows, seq_length * len(df.columns)]. \n",
    "    A sequence refers to how many should be appended as columns. \n",
    "    For example, if df = [['2020-01-01',100], ['2020-01-02', 101], ['2020-01-03', 102]] and seq_length = 2, the generated sequence will be [[101,102], [102,103]]. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    xs, ys = [], []\n",
    "    \n",
    "    if include_all_features == False:\n",
    "        for itr in range(num_rows - seq_length):\n",
    "            x = df.iloc[itr:(itr + seq_length), -1]\n",
    "            y = df.iloc[itr + seq_length, -1]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    elif include_all_features == True:\n",
    "        for itr in range(num_rows - seq_length):\n",
    "            x = df.iloc[itr:(itr + seq_length), :]\n",
    "            y = df.iloc[itr + seq_length, -1]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    else:\n",
    "        print(f\"error: include_all_features accepts True or False, got {include_all_features} instead.\")\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246d1ed0ddd399a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.661732900Z",
     "start_time": "2024-08-28T17:02:35.452166200Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training set\n",
    "sequence_len = 10\n",
    "num_rows = 200000\n",
    "X_train, y_train = create_sequences(df_target_train, sequence_len, num_rows)\n",
    "X_test, y_test = create_sequences(df_target_test, sequence_len, num_rows)\n",
    "\n",
    "X_train_v2, y_train_v2 = create_sequences(df_target_train_v2, sequence_len, num_rows, include_all_features=True)\n",
    "X_test_v2, y_test_v2 = create_sequences(df_target_test_v2, sequence_len, num_rows, include_all_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74cd4beed5c9b6cf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.712985400Z",
     "start_time": "2024-08-28T17:03:20.662733200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      open    high     low   close  volume  CPALTT01USM657N   DFF  EXPINF10YR  \\\n0   83.935  84.042  83.820  83.967     232          0.12452  0.19    1.574237   \n1   83.888  83.995  83.743  83.889     150          0.12452  0.19    1.574237   \n2   83.888  83.995  83.774  83.920    1600          0.12452  0.19    1.574237   \n3   83.904  84.011  83.776  83.923     100          0.12452  0.19    1.574237   \n4   83.966  84.073  83.851  83.998     698          0.12452  0.19    1.574237   \n..     ...     ...     ...     ...     ...              ...   ...         ...   \n95  83.577  83.684  83.463  83.609      50          0.12452  0.19    1.574237   \n96  83.608  83.715  83.494  83.640      93          0.12452  0.19    1.574237   \n97  83.593  83.700  83.479  83.625     100          0.12452  0.19    1.574237   \n98  83.733  83.840  83.618  83.765     100          0.12452  0.19    1.574237   \n99  83.779  83.886  83.665  83.811     103          0.12452  0.19    1.574237   \n\n        GDPC1     RSXFS  T10YFF  UNRATE    macd  macd_signal  macd_hist  \\\n0   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n1   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n2   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n3   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n4   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n..        ...       ...     ...     ...     ...          ...        ...   \n95  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n96  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n97  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n98  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n99  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n\n    daily_obv  seven_day_ema   close  target  \n0   -148100.0        81.6086  83.967       0  \n1   -148100.0        81.6086  83.889       1  \n2   -148100.0        81.6086  83.920       1  \n3   -148100.0        81.6086  83.923       1  \n4   -148100.0        81.6086  83.998       1  \n..        ...            ...     ...     ...  \n95  -165200.0        81.7122  83.609       1  \n96  -165200.0        81.7122  83.640       0  \n97  -165200.0        81.7122  83.625       1  \n98  -165200.0        81.7122  83.765       1  \n99  -165200.0        81.7122  83.811       1  \n\n[100 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>CPALTT01USM657N</th>\n      <th>DFF</th>\n      <th>EXPINF10YR</th>\n      <th>GDPC1</th>\n      <th>RSXFS</th>\n      <th>T10YFF</th>\n      <th>UNRATE</th>\n      <th>macd</th>\n      <th>macd_signal</th>\n      <th>macd_hist</th>\n      <th>daily_obv</th>\n      <th>seven_day_ema</th>\n      <th>close</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>83.935</td>\n      <td>84.042</td>\n      <td>83.820</td>\n      <td>83.967</td>\n      <td>232</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>83.888</td>\n      <td>83.995</td>\n      <td>83.743</td>\n      <td>83.889</td>\n      <td>150</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.889</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>83.888</td>\n      <td>83.995</td>\n      <td>83.774</td>\n      <td>83.920</td>\n      <td>1600</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.920</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>83.904</td>\n      <td>84.011</td>\n      <td>83.776</td>\n      <td>83.923</td>\n      <td>100</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.923</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>83.966</td>\n      <td>84.073</td>\n      <td>83.851</td>\n      <td>83.998</td>\n      <td>698</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.998</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>83.577</td>\n      <td>83.684</td>\n      <td>83.463</td>\n      <td>83.609</td>\n      <td>50</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.609</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>83.608</td>\n      <td>83.715</td>\n      <td>83.494</td>\n      <td>83.640</td>\n      <td>93</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.640</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>83.593</td>\n      <td>83.700</td>\n      <td>83.479</td>\n      <td>83.625</td>\n      <td>100</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.625</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>83.733</td>\n      <td>83.840</td>\n      <td>83.618</td>\n      <td>83.765</td>\n      <td>100</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.765</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>83.779</td>\n      <td>83.886</td>\n      <td>83.665</td>\n      <td>83.811</td>\n      <td>103</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.811</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 19 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[non_target_columns].head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a482e6f28809fe93",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.717945800Z",
     "start_time": "2024-08-28T17:03:20.711986100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 1., 1., 1., 1., 0., 1., 1., 0.])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf5237b9b95ddb6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.719453Z",
     "start_time": "2024-08-28T17:03:20.716174900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ef6c587bfbc278",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.725159Z",
     "start_time": "2024-08-28T17:03:20.719453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 8.39350000e+01,  8.40420000e+01,  8.38200000e+01,\n         8.39670000e+01,  2.32000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39670000e+01,\n         0.00000000e+00],\n       [ 8.38880000e+01,  8.39950000e+01,  8.37430000e+01,\n         8.38890000e+01,  1.50000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.38890000e+01,\n         1.00000000e+00],\n       [ 8.38880000e+01,  8.39950000e+01,  8.37740000e+01,\n         8.39200000e+01,  1.60000000e+03,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39200000e+01,\n         1.00000000e+00],\n       [ 8.39040000e+01,  8.40110000e+01,  8.37760000e+01,\n         8.39230000e+01,  1.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39230000e+01,\n         1.00000000e+00],\n       [ 8.39660000e+01,  8.40730000e+01,  8.38510000e+01,\n         8.39980000e+01,  6.98000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39980000e+01,\n         1.00000000e+00],\n       [ 8.41680000e+01,  8.42760000e+01,  8.40530000e+01,\n         8.42000000e+01,  1.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.42000000e+01,\n         1.00000000e+00],\n       [ 8.42770000e+01,  8.43850000e+01,  8.41620000e+01,\n         8.43090000e+01,  3.50000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.43090000e+01,\n         0.00000000e+00],\n       [ 8.41370000e+01,  8.42440000e+01,  8.40220000e+01,\n         8.41690000e+01,  1.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.41690000e+01,\n         1.00000000e+00],\n       [ 8.41520000e+01,  8.42600000e+01,  8.40370000e+01,\n         8.41840000e+01,  3.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.41840000e+01,\n         1.00000000e+00],\n       [ 8.42460000e+01,  8.43530000e+01,  8.41310000e+01,\n         8.42780000e+01,  2.35000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.42780000e+01,\n         0.00000000e+00]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "639c00fe086fe9e3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.727489400Z",
     "start_time": "2024-08-28T17:03:20.723130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_v2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd94e73620d73161",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:20.728998900Z",
     "start_time": "2024-08-28T17:03:20.727162800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199990 199990\n",
      "199990 199990\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train_v2))\n",
    "print(len(y_train), len(y_train_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c459d4662a2e0b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# using all fields available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "382fadba79bfaaee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:21.908445Z",
     "start_time": "2024-08-28T17:03:20.728998900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199990, 10, 19]) torch.Size([199990])\n",
      "input sample: tensor([[ 0.3158,  0.3158,  0.3158,  0.3158,  0.1254,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3158,  0.0000],\n",
      "        [ 0.3156,  0.3156,  0.3155,  0.3155,  0.0811,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3155,  0.3780],\n",
      "        [ 0.3156,  0.3156,  0.3156,  0.3156,  0.8650,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3156,  0.3780],\n",
      "        [ 0.3157,  0.3157,  0.3156,  0.3156,  0.0541,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3156,  0.3780],\n",
      "        [ 0.3159,  0.3159,  0.3159,  0.3159,  0.3774,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3159,  0.3780],\n",
      "        [ 0.3166,  0.3167,  0.3167,  0.3167,  0.0541,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3167,  0.3780],\n",
      "        [ 0.3171,  0.3171,  0.3171,  0.3171,  0.1892,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3171,  0.0000],\n",
      "        [ 0.3165,  0.3165,  0.3165,  0.3165,  0.0541,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3165,  0.3780],\n",
      "        [ 0.3166,  0.3166,  0.3166,  0.3166,  0.1622,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3166,  0.3780],\n",
      "        [ 0.3169,  0.3169,  0.3170,  0.3170,  0.1271,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3170,  0.0000]])\n",
      "label sample: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# convert df into objects Torch can read\n",
    "torch_X_train  = torch.from_numpy(X_train_v2).float()\n",
    "torch_y_train = torch.from_numpy(y_train_v2).float()\n",
    "torch_X_test = torch.from_numpy(X_test_v2).float()\n",
    "torch_y_test = torch.from_numpy(y_test_v2).float()\n",
    "\n",
    "# normalize data\n",
    "torch_X_train = torch.nn.functional.normalize(torch_X_train)\n",
    "torch_X_test = torch.nn.functional.normalize(torch_X_test)\n",
    "\n",
    "print(torch_X_train.shape, torch_y_train.shape)\n",
    "# create test and train sets\n",
    "train_data_set = TensorDataset(torch_X_train, torch_y_train)\n",
    "test_data_set = TensorDataset(torch_X_test, torch_y_test)\n",
    "\n",
    "# confirm it works\n",
    "sample = train_data_set[0]\n",
    "input_sample, label_sample = sample\n",
    "print('input sample:', input_sample)\n",
    "print('label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a58c3ea66f325d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using just the close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d2b0b4d1930876e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:21.915360700Z",
     "start_time": "2024-08-28T17:03:21.908445Z"
    }
   },
   "outputs": [],
   "source": [
    "# # convert df into objects Torch can read\n",
    "# torch_X_train  = torch.from_numpy(X_train).float()\n",
    "# torch_y_train = torch.from_numpy(y_train).float()\n",
    "# torch_X_test = torch.from_numpy(X_test).float()\n",
    "# torch_y_test = torch.from_numpy(y_test).float()\n",
    "# \n",
    "# # create test and train sets\n",
    "# train_data_set = TensorDataset(torch_X_train, torch_y_train)\n",
    "# test_data_set = TensorDataset(torch_X_test, torch_y_test)\n",
    "# \n",
    "# # confirm it works\n",
    "# sample = train_data_set[0]\n",
    "# input_sample, label_sample = sample\n",
    "# print('input sample:', input_sample)\n",
    "# print('label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea83ed54f081155",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:21.915360700Z",
     "start_time": "2024-08-28T17:03:21.911063800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 19])\n"
     ]
    }
   ],
   "source": [
    "print(input_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf6b9b42eb912d8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:21.916364300Z",
     "start_time": "2024-08-28T17:03:21.913565500Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = len(df_target_train_v2.columns)\n",
    "batch_size = 1000\n",
    "shuffle = False\n",
    "hidden_size = round(num_features / 2)\n",
    "num_layers = 2\n",
    "dropout = .25\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 50\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_data_set, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data_set, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "# test loader\n",
    "# x, y = next(iter(train_dataloader))\n",
    "# \n",
    "# print('x', x, 'y', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39b5019ca0b5a26c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:03:21.921287400Z",
     "start_time": "2024-08-28T17:03:21.916364300Z"
    }
   },
   "outputs": [],
   "source": [
    "# LTSM\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__() #super makes all the methods available in nn.Module available for the new class Net\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid() # we want a binary output, not %\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6d6aae47a1a113e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:37.353162800Z",
     "start_time": "2024-08-28T17:03:21.919284100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 682.99072265625, Accuracy: 0.5692110657691956\n",
      "Epoch 2, Loss: 682.9376220703125, Accuracy: 0.5692110657691956\n",
      "Epoch 3, Loss: 682.9298706054688, Accuracy: 0.5692110657691956\n",
      "Epoch 4, Loss: 682.9274291992188, Accuracy: 0.5692110657691956\n",
      "Epoch 5, Loss: 682.9258422851562, Accuracy: 0.5692110657691956\n",
      "Epoch 6, Loss: 682.9244384765625, Accuracy: 0.5692110657691956\n",
      "Epoch 7, Loss: 682.9229125976562, Accuracy: 0.5692110657691956\n",
      "Epoch 8, Loss: 682.9215087890625, Accuracy: 0.5692110657691956\n",
      "Epoch 9, Loss: 682.9199829101562, Accuracy: 0.5692110657691956\n",
      "Epoch 10, Loss: 682.91845703125, Accuracy: 0.5692110657691956\n",
      "Epoch 11, Loss: 682.9170532226562, Accuracy: 0.5692110657691956\n",
      "Epoch 12, Loss: 682.9156494140625, Accuracy: 0.5692110657691956\n",
      "Epoch 13, Loss: 682.9143676757812, Accuracy: 0.5692110657691956\n",
      "Epoch 14, Loss: 682.9132080078125, Accuracy: 0.5692110657691956\n",
      "Epoch 15, Loss: 682.912109375, Accuracy: 0.5692110657691956\n",
      "Epoch 16, Loss: 682.9111938476562, Accuracy: 0.5692110657691956\n",
      "Epoch 17, Loss: 682.9103393554688, Accuracy: 0.5692110657691956\n",
      "Epoch 18, Loss: 682.9094848632812, Accuracy: 0.5692110657691956\n",
      "Epoch 19, Loss: 682.9087524414062, Accuracy: 0.5692110657691956\n",
      "Epoch 20, Loss: 682.9080810546875, Accuracy: 0.5692110657691956\n",
      "Epoch 21, Loss: 682.907470703125, Accuracy: 0.5692110657691956\n",
      "Epoch 22, Loss: 682.9069213867188, Accuracy: 0.5692110657691956\n",
      "Epoch 23, Loss: 682.9063720703125, Accuracy: 0.5692110657691956\n",
      "Epoch 24, Loss: 682.9059448242188, Accuracy: 0.5692110657691956\n",
      "Epoch 25, Loss: 682.905517578125, Accuracy: 0.5692110657691956\n",
      "Epoch 26, Loss: 682.9052124023438, Accuracy: 0.5692110657691956\n",
      "Epoch 27, Loss: 682.9048461914062, Accuracy: 0.5692110657691956\n",
      "Epoch 28, Loss: 682.9046630859375, Accuracy: 0.5692110657691956\n",
      "Epoch 29, Loss: 682.9044799804688, Accuracy: 0.5692110657691956\n",
      "Epoch 30, Loss: 682.904296875, Accuracy: 0.5692110657691956\n",
      "Epoch 31, Loss: 682.9042358398438, Accuracy: 0.5692110657691956\n",
      "Epoch 32, Loss: 682.9041748046875, Accuracy: 0.5692110657691956\n",
      "Epoch 33, Loss: 682.9041748046875, Accuracy: 0.5692110657691956\n",
      "Epoch 34, Loss: 682.9041748046875, Accuracy: 0.5692110657691956\n",
      "Epoch 35, Loss: 682.904296875, Accuracy: 0.5692110657691956\n",
      "Epoch 36, Loss: 682.904541015625, Accuracy: 0.5692110657691956\n",
      "Epoch 37, Loss: 682.9047241210938, Accuracy: 0.5692110657691956\n",
      "Epoch 38, Loss: 682.905029296875, Accuracy: 0.5692110657691956\n",
      "Epoch 39, Loss: 682.9052734375, Accuracy: 0.5692110657691956\n",
      "Epoch 40, Loss: 682.9056396484375, Accuracy: 0.5692110657691956\n",
      "Epoch 41, Loss: 682.9059448242188, Accuracy: 0.5692110657691956\n",
      "Epoch 42, Loss: 682.9064331054688, Accuracy: 0.5692110657691956\n",
      "Epoch 43, Loss: 682.90673828125, Accuracy: 0.5692110657691956\n",
      "Epoch 44, Loss: 682.9071655273438, Accuracy: 0.5692110657691956\n",
      "Epoch 45, Loss: 682.9076538085938, Accuracy: 0.5692110657691956\n",
      "Epoch 46, Loss: 682.907958984375, Accuracy: 0.5692110657691956\n",
      "Epoch 47, Loss: 682.908447265625, Accuracy: 0.5692110657691956\n",
      "Epoch 48, Loss: 682.9088745117188, Accuracy: 0.5692110657691956\n",
      "Epoch 49, Loss: 682.9092407226562, Accuracy: 0.5692110657691956\n",
      "Epoch 50, Loss: 682.9095458984375, Accuracy: 0.5692110657691956\n"
     ]
    }
   ],
   "source": [
    "net = Net(input_size=num_features)\n",
    "criterion = nn.BCELoss(reduction='sum') # for binary prediction. Using the 'target' column\n",
    "#criterion = nn.MSELoss() # for regression, predicting the 'close' column\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(), lr=learning_rate\n",
    ")\n",
    "acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for dummy, data in enumerate(train_dataloader): # enumerate ensures that the dataloader always starts at the beginning since it's a generator object\n",
    "        torch.manual_seed(seed)\n",
    "        seqs = data[0].view(batch_size, sequence_len, num_features)\n",
    "        outputs = net(seqs).squeeze()\n",
    "        loss = criterion(outputs, data[1])\n",
    "        acc(torch.round(outputs), data[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {acc.compute()}\")\n",
    "    # early break if loss isn't changing beyond the learning rate\n",
    "    if loss.item() < .1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e992d7c8e1e29632",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:37.354163100Z",
     "start_time": "2024-08-28T17:05:37.348954600Z"
    }
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_size': [5, 10, num_features]\n",
    "#     , 'num_layers': range(4)\n",
    "#     , 'dropout': [.1, .5, .9]\n",
    "#     , 'learning_rate': [.0001, .0005, .001]\n",
    "#     , 'num_epochs': [10, 25, 50]}\n",
    "#\n",
    "# best_params = {'hidden_size': 0, 'num_layers': 0, 'dropout': 0, 'learning_rate': 0, 'num_epochs': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2790534f5428cde2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:37.354163100Z",
     "start_time": "2024-08-28T17:05:37.350720800Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define MSE metric\n",
    "# mse = torchmetrics.regression.MeanSquaredError()\n",
    "# \n",
    "# net.eval()\n",
    "# with torch.no_grad():\n",
    "#     for seqs, labels in test_dataloader:\n",
    "#         seqs = seqs.view(batch_size, torch_X_test.shape[1], num_features)\n",
    "#         # Pass seqs to net and squeeze the result\n",
    "#         outputs = net(seqs).squeeze()\n",
    "#         mse(outputs, labels)\n",
    "# \n",
    "# # Compute final metric value\n",
    "# test_mse = mse.compute()\n",
    "# print(f\"Test MSE: {test_mse}\")\n",
    "# print(f\"Test RMSE: {test_mse**.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae6937e7eff7177e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:39.418164400Z",
     "start_time": "2024-08-28T17:05:37.353162800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.5680500268936157\n"
     ]
    }
   ],
   "source": [
    "# test score\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for dummy, data in enumerate(test_dataloader):\n",
    "        torch.manual_seed(seed)\n",
    "        seqs = data[0].view(batch_size, sequence_len, num_features)\n",
    "        outputs = net(seqs).squeeze()\n",
    "        acc(torch.round(outputs), data[1])\n",
    "\n",
    "print(f\"Test accuracy score: {acc.compute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd07f53bb703ab5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Results\n",
    "\n",
    "Baseline guessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2449bac3ca1f312",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:39.419665800Z",
     "start_time": "2024-08-28T17:05:39.414924300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random guess likelihood 0.4687579112426077\n"
     ]
    }
   ],
   "source": [
    "random_guess_likelihood = df.target.mean()\n",
    "# mean = percent the candle was higher than the previous candle\n",
    "\n",
    "print('Random guess likelihood', random_guess_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdb8b68db71c78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Writeup\n",
    "At the minute level, 46.875% of the time the next close price is higher than the previous close price for VOO.\n",
    "The model is able to correctly predict a candle 56.3131% of the time making it better than a random (weighted) coinflip. The thing holding the model back is that it doesn't take into account expected payoff. The average positive candel is higher than the average negative candle in absolute terms. The model is doing almost exactly as well on the train and test set so there is not any overfitting going on. The accuracy is a bit low though so even though it's a nn it has high bias and underfitting overall.\n",
    "\n",
    "\n",
    "I used the following hyperparameters:\n",
    "sequence_len = 10\n",
    "* Sequence of length 10 seemed to give sufficient information without overwhelming my computer. Each input becomes sequence_len * feature so the compute cost is quite high increasing this value\n",
    "num_rows = 100000\n",
    "* This is to limit how much data I'm using. Adding more data doesn't seem to improve accuracy but it does crash performance.\n",
    "batch_size = 1000\n",
    "* Mostly arbitarily chosen. 1000 makes the model run faster and doesn't seem to sacrifice accuracy either.\n",
    "shuffle = False\n",
    "* don't want to shuffle sequence data since that defeats the purpose\n",
    "hidden_size = round(num_features / 2)\n",
    "* Heuristic that the best hidden_size is based on the average between the input and output sizes. Here my output is just 1 so it's just the number of features divided by 2\n",
    "num_layers = 2\n",
    "* Again heuristic, 2 hidden layers should be able to solve anything\n",
    "dropout = .5\n",
    "* Another mostly random chosen value. Playing around with this doesn't impact the model too much.\n",
    "\n",
    "learning_rate = 0.0001\n",
    "* small enough to change directions but not too small to where it doesn't update\n",
    "num_epochs = 50\n",
    "* Probably don't need this many. It stops really changing much after like 3-4 epochs\n",
    "\n",
    "\n",
    "I chose not to do dimensionality reduction for the LSTM model. I'm allowing the model to figure out which features and relationships are important or not. A big part of the value of deep learning is that it doesn't require me doing any sort of feature engineering or reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b082f8cb1131fb9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:39.421174900Z",
     "start_time": "2024-08-28T17:05:39.419168Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a562be68b27785df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce3841bd6b6833d5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:39.425382200Z",
     "start_time": "2024-08-28T17:05:39.421174900Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, num_epochs, batch_size):\n",
    "    criterion = nn.BCELoss(reduction='sum') # for binary prediction. Using the 'target' column\n",
    "    #criterion = nn.MSELoss() # for regression, predicting the 'close' column\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        for dummy, data in enumerate(train_dataloader):\n",
    "            model.train() # Since you called model.eval() above this is still the last mode it was set in. So you need to move it back to train\n",
    "            torch.manual_seed(seed)\n",
    "            seqs = data[0].view(batch_size, sequence_len, num_features)\n",
    "            outputs = model(seqs).squeeze()\n",
    "            loss = criterion(outputs, data[1])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # don't update the weights after each batch        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dummy, data in enumerate(test_dataloader):\n",
    "            torch.manual_seed(seed)\n",
    "            seqs = data[0].view(batch_size, sequence_len, num_features)\n",
    "            outputs = model(seqs).squeeze()\n",
    "            acc(torch.round(outputs), data[1])\n",
    "    \n",
    "    return acc.compute().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "758792ec4268e9ea",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:39.432402500Z",
     "start_time": "2024-08-28T17:05:39.424516300Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [.00001, .00005, .0001]\n",
    "batch_sizes = [1000, 5000]\n",
    "epochs = [2, 3, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87c869ed53b21a48",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:53.201988500Z",
     "start_time": "2024-08-28T17:05:39.428398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 1e-05 batch_size: 1000 ne: 2 current_score: 0.5669337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m test_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(test_data_set, batch_size\u001B[38;5;241m=\u001B[39mbs, shuffle\u001B[38;5;241m=\u001B[39mshuffle, drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m model \u001B[38;5;241m=\u001B[39m Net(input_size\u001B[38;5;241m=\u001B[39mnum_features)\n\u001B[1;32m---> 11\u001B[0m current_score \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mne\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Update best_score and best_params if current_score is better\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate:\u001B[39m\u001B[38;5;124m'\u001B[39m, lr, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size:\u001B[39m\u001B[38;5;124m'\u001B[39m, bs, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mne:\u001B[39m\u001B[38;5;124m'\u001B[39m, ne, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent_score:\u001B[39m\u001B[38;5;124m'\u001B[39m, current_score)\n",
      "Cell \u001B[1;32mIn[27], line 10\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(model, num_epochs, batch_size)\u001B[0m\n\u001B[0;32m      8\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[0;32m      9\u001B[0m seqs \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mview(batch_size, sequence_len, num_features)\n\u001B[1;32m---> 10\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseqs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, data[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     12\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\PycharmProjects\\trading\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\trading\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[21], line 18\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     16\u001B[0m h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(num_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), hidden_size)\n\u001B[0;32m     17\u001B[0m c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(num_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), hidden_size)\n\u001B[1;32m---> 18\u001B[0m out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :])\n\u001B[0;32m     20\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msigmoid(out)\n",
      "File \u001B[1;32m~\\PycharmProjects\\trading\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\trading\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\trading\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:917\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    914\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 917\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    920\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m    921\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_params = None\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for ne in epochs:\n",
    "            # Create DataLoaders with current batch_size\n",
    "            train_dataloader = DataLoader(train_data_set, batch_size=bs, shuffle=shuffle, drop_last=True)\n",
    "            test_dataloader = DataLoader(test_data_set, batch_size=bs, shuffle=shuffle, drop_last=True)\n",
    "            \n",
    "            model = Net(input_size=num_features)\n",
    "            current_score = train_and_evaluate(model, ne, bs)\n",
    "\n",
    "            # Update best_score and best_params if current_score is better\n",
    "            print('learning_rate:', lr, 'batch_size:', bs, 'ne:', ne, 'current_score:', current_score)\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_params = {'learning_rate': lr, 'batch_size': bs, 'num_epochs': ne}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best score:\", best_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e060a473e0ee1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:53.204989600Z",
     "start_time": "2024-08-28T17:05:53.201988500Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_best_model(model, best_params, train_data_set, test_data_set):\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data_set, batch_size=best_params['batch_size'], shuffle=False, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_data_set, batch_size=best_params['batch_size'], shuffle=False, drop_last=True)\n",
    "    \n",
    "    criterion = nn.BCELoss(reduction='sum') # for binary prediction. Using the 'target' column\n",
    "    #criterion = nn.MSELoss() # for regression, predicting the 'close' column\n",
    "    optimizer = optim.Adam(\n",
    "        net.parameters(), lr=best_params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for epoch in range(best_params['num_epochs']):\n",
    "        for dummy, data in enumerate(train_dataloader):\n",
    "            torch.manual_seed(seed)\n",
    "            model.train()\n",
    "            seqs = data[0].view(best_params['batch_size'], sequence_len, num_features)\n",
    "            outputs = model(seqs).squeeze()\n",
    "            loss = criterion(outputs, data[1])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dummy, data in enumerate(test_dataloader):\n",
    "            torch.manual_seed(seed)\n",
    "            seqs = data[0].view(best_params['batch_size'], sequence_len, num_features)\n",
    "            outputs = model(seqs).squeeze()\n",
    "            predictions.append(outputs)\n",
    "            \n",
    "    predictions = np.array(predictions).reshape(-1,1)\n",
    "    plt.plot(predictions)\n",
    "    plt.title(\"Stock Predictions\")\n",
    "    plt.xlabel(\"1-minute candles from 2020-01-01\")\n",
    "    plt.ylabel(\"Probability next close price is higher than the current close price\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47e0efbb6a079d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T17:05:53.202989600Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net(input_size=num_features)\n",
    "\n",
    "plot_best_model(net, best_params, train_data_set, test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1843a1516e14bd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T17:05:53.203989600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_target_test_v2.reset_index(inplace=True)\n",
    "df_target_test_v2['close'].iloc[:num_rows].plot(title=\"VOO Stock Price\", ylabel=\"Actual Stock Price\", xlabel=\"1-minute candle starting from 2020-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b99e925c1976e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Final Report\n",
    "\n",
    "Parameter tuning does very little to improve the model. The only thing that seems to make a difference is using a smaller number of epochs. To make this project shine a bit more, future work should include additional data gathering, especially data that isn't already strongly correlated with the existing features. There is also room to change this from a binary classification task to a regression task. What is currently lost is that not all positive or negative candles have the same magnitutde. Simply predicting up or down might have worse value than either a regression task or a multi classifier that could improve the rate of return on investments. Additionally, having a more zoomed out look might actually be more valuable. Instead of predicting each minute candle, predicting each hour or day may yield better results. I should also include training data much closer to the current year. I split based on pre 2020 and post 2020 which makes modern predictions less useful. \n",
    "\n",
    "The parameters I changed were: Best hyperparameters: {'learning_rate': 1e-05, 'batch_size': 1000, 'num_epochs': 2}\n",
    "\n",
    "\n",
    "It looks like the model doesn't do a better job than the baseline choosing to never buy. Since 52-53% of the time the close price is lower than the previous close price, the model seems to have picked that up and has almost the exact same prediction rate. Due to the nature of the problem, it seems that predictions seem to get stuck right below .5, which translates to always not buying. This does actually make sense. If the data is unable to yield any actionable patterns that would be better than the expected value of the stock price, then it's better just to pick the expected stock value which is just always 0 when rounded. While I'm not about to break any new ground in stock trading strategies, it does support a widely held belief that holding an SP500 ETF has a better payoff that just about any strategy. If this were converted from binary to regression I'd wager it would suggest always buying and not selling since that would have the highest total expected payout.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574de35395299d6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T17:05:53.203989600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c22a2c474f676",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T17:05:53.205989900Z",
     "start_time": "2024-08-28T17:05:53.204989600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447da99-eccf-412f-b635-7719475f1b24",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-28T17:05:53.205989900Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
