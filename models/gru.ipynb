{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a884471c8f9e99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-08-28T16:12:48.691517500Z",
     "start_time": "2024-08-28T16:12:47.656303300Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\connor\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TimeSeriesSplit\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\torch\\__init__.py:148\u001B[0m\n\u001B[0;32m    146\u001B[0m                 err \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mWinError(ctypes\u001B[38;5;241m.\u001B[39mget_last_error())\n\u001B[0;32m    147\u001B[0m                 err\u001B[38;5;241m.\u001B[39mstrerror \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m Error loading \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdll\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m or one of its dependencies.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 148\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[0;32m    150\u001B[0m     kernel32\u001B[38;5;241m.\u001B[39mSetErrorMode(prev_error_mode)\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_preload_cuda_deps\u001B[39m(lib_folder, lib_name):\n",
      "\u001B[1;31mOSError\u001B[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\connor\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54a882ded20973",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T16:12:48.697514Z",
     "start_time": "2024-08-28T16:12:48.692516100Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\connor\\PycharmProjects\\trading\\data\\analytics\\analytics_voo.csv'\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e113a86696fd67",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.693513700Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8b93f981cd0f5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.693513700Z"
    }
   },
   "outputs": [],
   "source": [
    "non_target_columns = ['open', 'high', 'low', 'close', 'volume', 'CPALTT01USM657N', 'DFF', 'EXPINF10YR', 'GDPC1', 'RSXFS', 'T10YFF', 'UNRATE', 'macd', 'macd_signal', 'macd_hist', 'daily_obv', 'seven_day_ema', 'close', 'target']\n",
    "\n",
    "df_cols = df[non_target_columns]\n",
    "\n",
    "# version with just close price and date\n",
    "df_target_train = df[['date', 'target']].where(df.date <= '2020-01-01')\n",
    "df_target_train.dropna(inplace=True)\n",
    "df_target_test = df[['date', 'target']].where(df.date > '2020-01-01')\n",
    "df_target_test.dropna(inplace=True)\n",
    "\n",
    "# version with all columns\n",
    "df_target_train_v2 = df[non_target_columns].where(df.date <= '2020-01-01')\n",
    "df_target_train_v2.dropna(inplace=True)\n",
    "df_target_test_v2 = df[non_target_columns].where(df.date > '2020-01-01')\n",
    "df_target_test_v2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c67e1f20487e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.694514600Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_target_train.columns)\n",
    "print()\n",
    "print(df_target_train_v2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13bd55cd725fa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.695513500Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_length, num_rows=len(df), include_all_features=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df: pandas dataframe with at least 2 columns, a date column and a target column\n",
    "    seq_length: How long the sequence should be. \n",
    "    num_rows: How many rows to use. Will pull all rows unless num_rows is provided, in which case the top num_rows will be pulled.\n",
    "    include_all_features: Whether to use just the target column for the sequence, or to include all features.\n",
    "        \n",
    "    Output:\n",
    "    Generates a sequence of seq_length length. It's shape is [num_rows, seq_length] if include_all_features is False, else [num_rows, seq_length * len(df.columns)]. \n",
    "    A sequence refers to how many should be appended as columns. \n",
    "    For example, if df = [['2020-01-01',100], ['2020-01-02', 101], ['2020-01-03', 102]] and seq_length = 2, the generated sequence will be [[101,102], [102,103]]. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    xs, ys = [], []\n",
    "    \n",
    "    if include_all_features == False:\n",
    "        for itr in range(num_rows - seq_length):\n",
    "            x = df.iloc[itr:(itr + seq_length), -1]\n",
    "            y = df.iloc[itr + seq_length, -1]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    elif include_all_features == True:\n",
    "        for itr in range(num_rows - seq_length):\n",
    "            x = df.iloc[itr:(itr + seq_length), :]\n",
    "            y = df.iloc[itr + seq_length, -1]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    else:\n",
    "        print(f\"error: include_all_features accepts True or False, got {include_all_features} instead.\")\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d1ed0ddd399a4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.696514Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training set\n",
    "sequence_len = 10\n",
    "num_rows = 100000\n",
    "X_train, y_train = create_sequences(df_target_train, sequence_len, num_rows)\n",
    "X_test, y_test = create_sequences(df_target_test, sequence_len, num_rows)\n",
    "\n",
    "X_train_v2, y_train_v2 = create_sequences(df_target_train_v2, sequence_len, num_rows, include_all_features=True)\n",
    "X_test_v2, y_test_v2 = create_sequences(df_target_test_v2, sequence_len, num_rows, include_all_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd4beed5c9b6cf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.696514Z"
    }
   },
   "outputs": [],
   "source": [
    "df[non_target_columns].head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482e6f28809fe93",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.697514Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5237b9b95ddb6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T16:12:48.699514200Z",
     "start_time": "2024-08-28T16:12:48.698514Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef6c587bfbc278",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.699514200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_v2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c00fe086fe9e3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.699514200Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_v2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94e73620d73161",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T16:12:48.706516800Z",
     "start_time": "2024-08-28T16:12:48.700514100Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_train_v2))\n",
    "print(len(y_train), len(y_train_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c459d4662a2e0b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# using all fields available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fadba79bfaaee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.701514Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert df into objects Torch can read\n",
    "torch_X_train  = torch.from_numpy(X_train_v2).float()\n",
    "torch_y_train = torch.from_numpy(y_train_v2).float()\n",
    "torch_X_test = torch.from_numpy(X_test_v2).float()\n",
    "torch_y_test = torch.from_numpy(y_test_v2).float()\n",
    "\n",
    "# normalize data\n",
    "torch_X_train = torch.nn.functional.normalize(torch_X_train)\n",
    "torch_X_test = torch.nn.functional.normalize(torch_X_test)\n",
    "\n",
    "print(torch_X_train.shape, torch_y_train.shape)\n",
    "# create test and train sets\n",
    "train_data_set = TensorDataset(torch_X_train, torch_y_train)\n",
    "test_data_set = TensorDataset(torch_X_test, torch_y_test)\n",
    "\n",
    "# confirm it works\n",
    "sample = train_data_set[0]\n",
    "input_sample, label_sample = sample\n",
    "print('input sample:', input_sample)\n",
    "print('label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a58c3ea66f325d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using just the close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b0b4d1930876e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.701514Z"
    }
   },
   "outputs": [],
   "source": [
    "# # convert df into objects Torch can read\n",
    "# torch_X_train  = torch.from_numpy(X_train).float()\n",
    "# torch_y_train = torch.from_numpy(y_train).float()\n",
    "# torch_X_test = torch.from_numpy(X_test).float()\n",
    "# torch_y_test = torch.from_numpy(y_test).float()\n",
    "# \n",
    "# # create test and train sets\n",
    "# train_data_set = TensorDataset(torch_X_train, torch_y_train)\n",
    "# test_data_set = TensorDataset(torch_X_test, torch_y_test)\n",
    "# \n",
    "# # confirm it works\n",
    "# sample = train_data_set[0]\n",
    "# input_sample, label_sample = sample\n",
    "# print('input sample:', input_sample)\n",
    "# print('label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea83ed54f081155",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.702516300Z"
    }
   },
   "outputs": [],
   "source": [
    "print(input_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992d7c8e1e29632",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.703515200Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = len(df_target_train_v2.columns)\n",
    "batch_size = 1000\n",
    "shuffle = False\n",
    "hidden_size = round(num_features / 2)\n",
    "num_layers = 2\n",
    "dropout = .5\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 50\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_data_set, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data_set, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "# test loader\n",
    "# x, y = next(iter(train_dataloader))\n",
    "# \n",
    "# print('x', x, 'y', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6aae47a1a113e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.703515200Z"
    }
   },
   "outputs": [],
   "source": [
    "# GRU\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__() #super makes all the methods available in nn.Module available for the new class Net\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid() # we want a binary output, not %\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790534f5428cde2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.704516800Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net(input_size=num_features)\n",
    "criterion = nn.BCELoss(reduction='sum') # for binary prediction. Using the 'target' column\n",
    "#criterion = nn.MSELoss() # for regression, predicting the 'close' column\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(), lr=learning_rate\n",
    ")\n",
    "acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for seqs, labels in train_dataloader:\n",
    "        seqs = seqs.view(batch_size, sequence_len, num_features)\n",
    "        outputs = net(seqs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc(torch.round(outputs), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {acc.compute()}\")\n",
    "    # early break if loss isn't changing beyond the learning rate\n",
    "    if loss.item() < .1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6937e7eff7177e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.705516400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define MSE metric\n",
    "# mse = torchmetrics.regression.MeanSquaredError()\n",
    "# \n",
    "# net.eval()\n",
    "# with torch.no_grad():\n",
    "#     for seqs, labels in test_dataloader:\n",
    "#         seqs = seqs.view(batch_size, torch_X_test.shape[1], num_features)\n",
    "#         # Pass seqs to net and squeeze the result\n",
    "#         outputs = net(seqs).squeeze()\n",
    "#         mse(outputs, labels)\n",
    "# \n",
    "# # Compute final metric value\n",
    "# test_mse = mse.compute()\n",
    "# print(f\"Test MSE: {test_mse}\")\n",
    "# print(f\"Test RMSE: {test_mse**.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9824920abf3b48d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.705516400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test set\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in test_dataloader:\n",
    "        seqs = seqs.view(batch_size, sequence_len, num_features)\n",
    "        outputs = net(seqs).squeeze()\n",
    "        acc(torch.round(outputs), labels)\n",
    "\n",
    "print(f\"Test accuracy score: {acc.compute()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd50b5d6e213ee0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Results\n",
    "\n",
    "Baseline guessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd08bdbd74c444",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-28T16:12:48.706516800Z"
    }
   },
   "outputs": [],
   "source": [
    "random_guess_likelihood = df.target.mean()\n",
    "# mean = percent the candle was higher than the previous candle\n",
    "\n",
    "print('Random guess likelihood', random_guess_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db37e546b6057c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Writeup\n",
    "At the minute level, 46.875% of the time the next close price is higher than the previous close price for VOO.\n",
    "The model is able to correctly predict a candle 56.201% of the time making it better than a random (weighted) coinflip. The thing holding the model back is that it doesn't take into account expected payoff. The average positive candel is higher than the average negative candle in absolute terms. The model is doing almost exactly as well on the train and test set so there is not any overfitting going on. The accuracy is a bit low though so even though it's a nn it has high bias and underfitting overall. It is just a hair less accurate than the LSTM.\n",
    "\n",
    "\n",
    "I used the following hyperparameters:\n",
    "sequence_len = 10\n",
    "* Sequence of length 10 seemed to give sufficient information without overwhelming my computer. Each input becomes sequence_len * feature so the compute cost is quite high increasing this value\n",
    "num_rows = 100000\n",
    "* This is to limit how much data I'm using. Adding more data doesn't seem to improve accuracy but it does crash performance.\n",
    "batch_size = 1000\n",
    "* Mostly arbitarily chosen. 1000 makes the model run faster and doesn't seem to sacrifice accuracy either.\n",
    "shuffle = False\n",
    "* don't want to shuffle sequence data since that defeats the purpose\n",
    "hidden_size = round(num_features / 2)\n",
    "* Heuristic that the best hidden_size is based on the average between the input and output sizes. Here my output is just 1 so it's just the number of features divided by 2\n",
    "num_layers = 2\n",
    "* Again heuristic, 2 hidden layers should be able to solve anything\n",
    "dropout = .5\n",
    "* Another mostly random chosen value. Playing around with this doesn't impact the model too much.\n",
    "\n",
    "learning_rate = 0.0001\n",
    "* small enough to change directions but not too small to where it doesn't update\n",
    "num_epochs = 50\n",
    "* Probably don't need this many. It stops really changing much after like 3-4 epochs\n",
    "\n",
    "\n",
    "I chose not to do dimensionality reduction for the GRU model. I'm allowing the model to figure out which features and relationships are important or not. A big part of the value of deep learning is that it doesn't require me doing any sort of feature engineering or reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacc64fdee8f2c2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-28T16:12:48.709516400Z",
     "start_time": "2024-08-28T16:12:48.706516800Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
