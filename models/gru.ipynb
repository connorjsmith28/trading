{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7a884471c8f9e99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NOTES\n",
    "Right now only takes close price. Update to include all data in the sequence.\n",
    "\n",
    "After 3 epochs it's loss is basically non-existant. Use an early stop\n",
    "\n",
    "Learn how to use GPU \n",
    "\n",
    "drop_last=True seems necessary for the data loader. The batch_size argument is expecting batches to be that size. But if the whole dataset can't be divided by the batch_size then you had some values remaining that don't fit. Could set batch_size to be some multiple if possible to minimize dropped samples \n",
    "\n",
    "It seems to just be predicting the same thing\n",
    "    try increasing batch size\n",
    "    overfit on 1-2 samples first"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c3094fd6965f05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchmetrics"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:22:34.164031200Z",
     "start_time": "2024-07-31T15:22:34.118523400Z"
    }
   },
   "id": "initial_id",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DATA_PATH = r'C:\\Users\\connor\\PycharmProjects\\trading\\data\\analytics\\analytics_voo.csv'\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:22:39.667608Z",
     "start_time": "2024-07-31T15:22:34.130031200Z"
    }
   },
   "id": "fe54a882ded20973",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['date', 'open', 'high', 'low', 'close', 'volume', 'join_date', 'Id',\n       'Date', 'DateLongDescription', 'DateShortDescription', 'DayLongName',\n       'DayShortName', 'MonthLongName', 'MonthShortName', 'CalendarDay',\n       'CalendarWeek', 'CalendarWeekStartDateId', 'CalendarWeekEndDateId',\n       'CalendarDayInWeek', 'CalendarMonth', 'CalendarMonthStartDateId',\n       'CalendarMonthEndDateId', 'CalendarNumberOfDaysInMonth',\n       'CalendarDayInMonth', 'CalendarQuarter', 'CalendarQuarterStartDateId',\n       'CalendarQuarterEndDateId', 'CalendarQuarterStartDate',\n       'CalendarNumberOfDaysInQuarter', 'CalendarDayInQuarter', 'CalendarYear',\n       'CalendarYearEndDateId', 'CalendarYearStartDate',\n       'CalendarNumberOfDaysInYear', 'month_join_key', 'year_join_key',\n       'seven_day_ema', 'CPALTT01USM657N', 'DFF', 'EXPINF10YR', 'GDPC1',\n       'RSXFS', 'T10YFF', 'UNRATE', 'macd', 'macd_signal', 'macd_hist',\n       'daily_obv', 'target'],\n      dtype='object')"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:22:39.676563600Z",
     "start_time": "2024-07-31T15:22:39.667608Z"
    }
   },
   "id": "84e113a86696fd67",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "non_target_columns = ['open', 'high', 'low', 'close', 'volume', 'CPALTT01USM657N', 'DFF', 'EXPINF10YR', 'GDPC1', 'RSXFS', 'T10YFF', 'UNRATE', 'macd', 'macd_signal', 'macd_hist', 'daily_obv', 'seven_day_ema', 'close', 'target']\n",
    "\n",
    "df_cols = df[non_target_columns]\n",
    "\n",
    "# version with just close price and date\n",
    "df_target_train = df[['date', 'target']].where(df.date <= '2020-01-01')\n",
    "df_target_train.dropna(inplace=True)\n",
    "df_target_test = df[['date', 'target']].where(df.date > '2020-01-01')\n",
    "df_target_test.dropna(inplace=True)\n",
    "\n",
    "# version with all columns\n",
    "df_target_train_v2 = df[non_target_columns].where(df.date <= '2020-01-01')\n",
    "df_target_train_v2.dropna(inplace=True)\n",
    "df_target_test_v2 = df[non_target_columns].where(df.date > '2020-01-01')\n",
    "df_target_test_v2.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:22:40.440171100Z",
     "start_time": "2024-07-31T15:22:39.670562Z"
    }
   },
   "id": "d1c8b93f981cd0f5",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'target'], dtype='object')\n",
      "\n",
      "Index(['open', 'high', 'low', 'close', 'volume', 'CPALTT01USM657N', 'DFF',\n",
      "       'EXPINF10YR', 'GDPC1', 'RSXFS', 'T10YFF', 'UNRATE', 'macd',\n",
      "       'macd_signal', 'macd_hist', 'daily_obv', 'seven_day_ema', 'close',\n",
      "       'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_target_train.columns)\n",
    "print()\n",
    "print(df_target_train_v2.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:22:40.445722200Z",
     "start_time": "2024-07-31T15:22:40.441171300Z"
    }
   },
   "id": "4b6c67e1f20487e",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_length, num_rows=len(df), include_all_features=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df: pandas dataframe with at least 2 columns, a date column and a target column\n",
    "    seq_length: How long the sequence should be. \n",
    "    num_rows: How many rows to use. Will pull all rows unless num_rows is provided, in which case the top num_rows will be pulled.\n",
    "    include_all_features: Whether to use just the target column for the sequence, or to include all features.\n",
    "        \n",
    "    Output:\n",
    "    Generates a sequence of seq_length length. It's shape is [num_rows, seq_length] if include_all_features is False, else [num_rows, seq_length * len(df.columns)]. \n",
    "    A sequence refers to how many should be appended as columns. \n",
    "    For example, if df = [['2020-01-01',100], ['2020-01-02', 101], ['2020-01-03', 102]] and seq_length = 2, the generated sequence will be [[101,102], [102,103]]. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    xs, ys = [], []\n",
    "    \n",
    "    if include_all_features == False:\n",
    "        for itr in range(num_rows - seq_length):\n",
    "            x = df.iloc[itr:(itr + seq_length), -1]\n",
    "            y = df.iloc[itr + seq_length, -1]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "    elif include_all_features == True:\n",
    "        for itr in range(num_rows - seq_length):\n",
    "            x = df.iloc[itr:(itr + seq_length), :]\n",
    "            y = df.iloc[itr + seq_length, -1]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    else:\n",
    "        print(f\"error: include_all_features accepts True or False, got {include_all_features} instead.\")\n",
    "    return np.array(xs), np.array(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:22:40.446726900Z",
     "start_time": "2024-07-31T15:22:40.444362300Z"
    }
   },
   "id": "7a13bd55cd725fa",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create training set\n",
    "sequence_len = 10\n",
    "num_rows = 100000\n",
    "X_train, y_train = create_sequences(df_target_train, sequence_len, num_rows)\n",
    "X_test, y_test = create_sequences(df_target_test, sequence_len, num_rows)\n",
    "\n",
    "X_train_v2, y_train_v2 = create_sequences(df_target_train_v2, sequence_len, num_rows, include_all_features=True)\n",
    "X_test_v2, y_test_v2 = create_sequences(df_target_test_v2, sequence_len, num_rows, include_all_features=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.025502700Z",
     "start_time": "2024-07-31T15:22:40.447728500Z"
    }
   },
   "id": "246d1ed0ddd399a4",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      open    high     low   close  volume  CPALTT01USM657N   DFF  EXPINF10YR  \\\n0   83.935  84.042  83.820  83.967     232          0.12452  0.19    1.574237   \n1   83.888  83.995  83.743  83.889     150          0.12452  0.19    1.574237   \n2   83.888  83.995  83.774  83.920    1600          0.12452  0.19    1.574237   \n3   83.904  84.011  83.776  83.923     100          0.12452  0.19    1.574237   \n4   83.966  84.073  83.851  83.998     698          0.12452  0.19    1.574237   \n..     ...     ...     ...     ...     ...              ...   ...         ...   \n95  83.577  83.684  83.463  83.609      50          0.12452  0.19    1.574237   \n96  83.608  83.715  83.494  83.640      93          0.12452  0.19    1.574237   \n97  83.593  83.700  83.479  83.625     100          0.12452  0.19    1.574237   \n98  83.733  83.840  83.618  83.765     100          0.12452  0.19    1.574237   \n99  83.779  83.886  83.665  83.811     103          0.12452  0.19    1.574237   \n\n        GDPC1     RSXFS  T10YFF  UNRATE    macd  macd_signal  macd_hist  \\\n0   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n1   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n2   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n3   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n4   16960.864  323990.0    2.48     9.4  1.1427       1.1947    -0.0521   \n..        ...       ...     ...     ...     ...          ...        ...   \n95  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n96  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n97  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n98  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n99  16960.864  323990.0    2.56     9.4  1.1110       1.1834    -0.0724   \n\n    daily_obv  seven_day_ema   close  target  \n0   -148100.0        81.6086  83.967       0  \n1   -148100.0        81.6086  83.889       1  \n2   -148100.0        81.6086  83.920       1  \n3   -148100.0        81.6086  83.923       1  \n4   -148100.0        81.6086  83.998       1  \n..        ...            ...     ...     ...  \n95  -165200.0        81.7122  83.609       1  \n96  -165200.0        81.7122  83.640       0  \n97  -165200.0        81.7122  83.625       1  \n98  -165200.0        81.7122  83.765       1  \n99  -165200.0        81.7122  83.811       1  \n\n[100 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>CPALTT01USM657N</th>\n      <th>DFF</th>\n      <th>EXPINF10YR</th>\n      <th>GDPC1</th>\n      <th>RSXFS</th>\n      <th>T10YFF</th>\n      <th>UNRATE</th>\n      <th>macd</th>\n      <th>macd_signal</th>\n      <th>macd_hist</th>\n      <th>daily_obv</th>\n      <th>seven_day_ema</th>\n      <th>close</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>83.935</td>\n      <td>84.042</td>\n      <td>83.820</td>\n      <td>83.967</td>\n      <td>232</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>83.888</td>\n      <td>83.995</td>\n      <td>83.743</td>\n      <td>83.889</td>\n      <td>150</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.889</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>83.888</td>\n      <td>83.995</td>\n      <td>83.774</td>\n      <td>83.920</td>\n      <td>1600</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.920</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>83.904</td>\n      <td>84.011</td>\n      <td>83.776</td>\n      <td>83.923</td>\n      <td>100</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.923</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>83.966</td>\n      <td>84.073</td>\n      <td>83.851</td>\n      <td>83.998</td>\n      <td>698</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.48</td>\n      <td>9.4</td>\n      <td>1.1427</td>\n      <td>1.1947</td>\n      <td>-0.0521</td>\n      <td>-148100.0</td>\n      <td>81.6086</td>\n      <td>83.998</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>83.577</td>\n      <td>83.684</td>\n      <td>83.463</td>\n      <td>83.609</td>\n      <td>50</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.609</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>83.608</td>\n      <td>83.715</td>\n      <td>83.494</td>\n      <td>83.640</td>\n      <td>93</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.640</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>83.593</td>\n      <td>83.700</td>\n      <td>83.479</td>\n      <td>83.625</td>\n      <td>100</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.625</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>83.733</td>\n      <td>83.840</td>\n      <td>83.618</td>\n      <td>83.765</td>\n      <td>100</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.765</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>83.779</td>\n      <td>83.886</td>\n      <td>83.665</td>\n      <td>83.811</td>\n      <td>103</td>\n      <td>0.12452</td>\n      <td>0.19</td>\n      <td>1.574237</td>\n      <td>16960.864</td>\n      <td>323990.0</td>\n      <td>2.56</td>\n      <td>9.4</td>\n      <td>1.1110</td>\n      <td>1.1834</td>\n      <td>-0.0724</td>\n      <td>-165200.0</td>\n      <td>81.7122</td>\n      <td>83.811</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 19 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[non_target_columns].head(n=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.069076Z",
     "start_time": "2024-07-31T15:23:05.025502700Z"
    }
   },
   "id": "74cd4beed5c9b6cf",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1., 1., 1., 1., 1., 0., 1., 1., 0.])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.077504300Z",
     "start_time": "2024-07-31T15:23:05.069076Z"
    }
   },
   "id": "a482e6f28809fe93",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.077504300Z",
     "start_time": "2024-07-31T15:23:05.072987600Z"
    }
   },
   "id": "ecf5237b9b95ddb6",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 8.39350000e+01,  8.40420000e+01,  8.38200000e+01,\n         8.39670000e+01,  2.32000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39670000e+01,\n         0.00000000e+00],\n       [ 8.38880000e+01,  8.39950000e+01,  8.37430000e+01,\n         8.38890000e+01,  1.50000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.38890000e+01,\n         1.00000000e+00],\n       [ 8.38880000e+01,  8.39950000e+01,  8.37740000e+01,\n         8.39200000e+01,  1.60000000e+03,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39200000e+01,\n         1.00000000e+00],\n       [ 8.39040000e+01,  8.40110000e+01,  8.37760000e+01,\n         8.39230000e+01,  1.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39230000e+01,\n         1.00000000e+00],\n       [ 8.39660000e+01,  8.40730000e+01,  8.38510000e+01,\n         8.39980000e+01,  6.98000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.39980000e+01,\n         1.00000000e+00],\n       [ 8.41680000e+01,  8.42760000e+01,  8.40530000e+01,\n         8.42000000e+01,  1.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.42000000e+01,\n         1.00000000e+00],\n       [ 8.42770000e+01,  8.43850000e+01,  8.41620000e+01,\n         8.43090000e+01,  3.50000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.43090000e+01,\n         0.00000000e+00],\n       [ 8.41370000e+01,  8.42440000e+01,  8.40220000e+01,\n         8.41690000e+01,  1.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.41690000e+01,\n         1.00000000e+00],\n       [ 8.41520000e+01,  8.42600000e+01,  8.40370000e+01,\n         8.41840000e+01,  3.00000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.41840000e+01,\n         1.00000000e+00],\n       [ 8.42460000e+01,  8.43530000e+01,  8.41310000e+01,\n         8.42780000e+01,  2.35000000e+02,  1.24519889e-01,\n         1.90000000e-01,  1.57423670e+00,  1.69608640e+04,\n         3.23990000e+05,  2.48000000e+00,  9.40000000e+00,\n         1.14270000e+00,  1.19470000e+00, -5.21000000e-02,\n        -1.48100000e+05,  8.16086000e+01,  8.42780000e+01,\n         0.00000000e+00]])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v2[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.080304900Z",
     "start_time": "2024-07-31T15:23:05.076503100Z"
    }
   },
   "id": "16ef6c587bfbc278",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_v2[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.093639500Z",
     "start_time": "2024-07-31T15:23:05.080304900Z"
    }
   },
   "id": "639c00fe086fe9e3",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99990 99990\n",
      "99990 99990\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train_v2))\n",
    "print(len(y_train), len(y_train_v2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.094639100Z",
     "start_time": "2024-07-31T15:23:05.084619100Z"
    }
   },
   "id": "fd94e73620d73161",
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "source": [
    "# using all fields available"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c459d4662a2e0b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99990, 10, 19]) torch.Size([99990])\n",
      "input sample: tensor([[ 0.3158,  0.3158,  0.3158,  0.3158,  0.1254,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3158,  0.0000],\n",
      "        [ 0.3156,  0.3156,  0.3155,  0.3155,  0.0811,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3155,  0.3780],\n",
      "        [ 0.3156,  0.3156,  0.3156,  0.3156,  0.8650,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3156,  0.3780],\n",
      "        [ 0.3157,  0.3157,  0.3156,  0.3156,  0.0541,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3156,  0.3780],\n",
      "        [ 0.3159,  0.3159,  0.3159,  0.3159,  0.3774,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3159,  0.3780],\n",
      "        [ 0.3166,  0.3167,  0.3167,  0.3167,  0.0541,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3167,  0.3780],\n",
      "        [ 0.3171,  0.3171,  0.3171,  0.3171,  0.1892,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3171,  0.0000],\n",
      "        [ 0.3165,  0.3165,  0.3165,  0.3165,  0.0541,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3165,  0.3780],\n",
      "        [ 0.3166,  0.3166,  0.3166,  0.3166,  0.1622,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3166,  0.3780],\n",
      "        [ 0.3169,  0.3169,  0.3170,  0.3170,  0.1271,  0.3162,  0.3162,  0.3162,\n",
      "          0.3162,  0.3162,  0.3162,  0.3162,  0.3162,  0.3162, -0.3162, -0.3162,\n",
      "          0.3162,  0.3170,  0.0000]])\n",
      "label sample: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# convert df into objects Torch can read\n",
    "torch_X_train  = torch.from_numpy(X_train_v2).float()\n",
    "torch_y_train = torch.from_numpy(y_train_v2).float()\n",
    "torch_X_test = torch.from_numpy(X_test_v2).float()\n",
    "torch_y_test = torch.from_numpy(y_test_v2).float()\n",
    "\n",
    "# normalize data\n",
    "torch_X_train = torch.nn.functional.normalize(torch_X_train)\n",
    "torch_X_test = torch.nn.functional.normalize(torch_X_test)\n",
    "\n",
    "print(torch_X_train.shape, torch_y_train.shape)\n",
    "# create test and train sets\n",
    "train_data_set = TensorDataset(torch_X_train, torch_y_train)\n",
    "test_data_set = TensorDataset(torch_X_test, torch_y_test)\n",
    "\n",
    "# confirm it works\n",
    "sample = train_data_set[0]\n",
    "input_sample, label_sample = sample\n",
    "print('input sample:', input_sample)\n",
    "print('label sample:', label_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.258120300Z",
     "start_time": "2024-07-31T15:23:05.087127800Z"
    }
   },
   "id": "382fadba79bfaaee",
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using just the close price"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a58c3ea66f325d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # convert df into objects Torch can read\n",
    "# torch_X_train  = torch.from_numpy(X_train).float()\n",
    "# torch_y_train = torch.from_numpy(y_train).float()\n",
    "# torch_X_test = torch.from_numpy(X_test).float()\n",
    "# torch_y_test = torch.from_numpy(y_test).float()\n",
    "# \n",
    "# # create test and train sets\n",
    "# train_data_set = TensorDataset(torch_X_train, torch_y_train)\n",
    "# test_data_set = TensorDataset(torch_X_test, torch_y_test)\n",
    "# \n",
    "# # confirm it works\n",
    "# sample = train_data_set[0]\n",
    "# input_sample, label_sample = sample\n",
    "# print('input sample:', input_sample)\n",
    "# print('label sample:', label_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.258120300Z",
     "start_time": "2024-07-31T15:23:05.253443100Z"
    }
   },
   "id": "8d2b0b4d1930876e",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 19])\n"
     ]
    }
   ],
   "source": [
    "print(input_sample.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.259120400Z",
     "start_time": "2024-07-31T15:23:05.255951600Z"
    }
   },
   "id": "7ea83ed54f081155",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_features = len(df_target_train_v2.columns)\n",
    "batch_size = 1000\n",
    "shuffle = False\n",
    "hidden_size = round(num_features / 2)\n",
    "num_layers = 2\n",
    "dropout = .5\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 50\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = DataLoader(train_data_set, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data_set, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "# test loader\n",
    "# x, y = next(iter(train_dataloader))\n",
    "# \n",
    "# print('x', x, 'y', y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.266432500Z",
     "start_time": "2024-07-31T15:23:05.258120300Z"
    }
   },
   "id": "e992d7c8e1e29632",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# GRU\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__() #super makes all the methods available in nn.Module available for the new class Net\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid() # we want a binary output, not %\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:23:05.267433100Z",
     "start_time": "2024-07-31T15:23:05.261563Z"
    }
   },
   "id": "d6d6aae47a1a113e",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 677.8063354492188, Accuracy: 0.5613737106323242\n",
      "Epoch 2, Loss: 675.26611328125, Accuracy: 0.5622525215148926\n",
      "Epoch 3, Loss: 674.687255859375, Accuracy: 0.5625454783439636\n",
      "Epoch 4, Loss: 672.8005981445312, Accuracy: 0.5626919269561768\n",
      "Epoch 5, Loss: 674.231689453125, Accuracy: 0.5627797842025757\n",
      "Epoch 6, Loss: 674.8531494140625, Accuracy: 0.5628383755683899\n",
      "Epoch 7, Loss: 675.1661987304688, Accuracy: 0.5628802180290222\n",
      "Epoch 8, Loss: 674.1546020507812, Accuracy: 0.5629116296768188\n",
      "Epoch 9, Loss: 674.9287109375, Accuracy: 0.562936007976532\n",
      "Epoch 10, Loss: 674.48193359375, Accuracy: 0.5629555583000183\n",
      "Epoch 11, Loss: 674.3342895507812, Accuracy: 0.5629715323448181\n",
      "Epoch 12, Loss: 673.4663696289062, Accuracy: 0.562984824180603\n",
      "Epoch 13, Loss: 673.829833984375, Accuracy: 0.5629960894584656\n",
      "Epoch 14, Loss: 675.5048828125, Accuracy: 0.5630057454109192\n",
      "Epoch 15, Loss: 674.4779663085938, Accuracy: 0.5630141496658325\n",
      "Epoch 16, Loss: 675.0918579101562, Accuracy: 0.5630214810371399\n",
      "Epoch 17, Loss: 674.9735107421875, Accuracy: 0.5630279183387756\n",
      "Epoch 18, Loss: 673.5966796875, Accuracy: 0.5630336403846741\n",
      "Epoch 19, Loss: 673.7084350585938, Accuracy: 0.5630388259887695\n",
      "Epoch 20, Loss: 673.6370849609375, Accuracy: 0.5630434155464172\n",
      "Epoch 21, Loss: 674.32275390625, Accuracy: 0.5630476474761963\n",
      "Epoch 22, Loss: 675.2627563476562, Accuracy: 0.5630514025688171\n",
      "Epoch 23, Loss: 673.8645629882812, Accuracy: 0.5630549192428589\n",
      "Epoch 24, Loss: 674.8864135742188, Accuracy: 0.563058078289032\n",
      "Epoch 25, Loss: 674.9149169921875, Accuracy: 0.563060998916626\n",
      "Epoch 26, Loss: 674.3326416015625, Accuracy: 0.5630637407302856\n",
      "Epoch 27, Loss: 674.8406982421875, Accuracy: 0.5630662441253662\n",
      "Epoch 28, Loss: 673.2191162109375, Accuracy: 0.5630685687065125\n",
      "Epoch 29, Loss: 673.7057495117188, Accuracy: 0.5630707144737244\n",
      "Epoch 30, Loss: 674.1973876953125, Accuracy: 0.5630727410316467\n",
      "Epoch 31, Loss: 674.1121826171875, Accuracy: 0.5630745887756348\n",
      "Epoch 32, Loss: 674.8043212890625, Accuracy: 0.563076376914978\n",
      "Epoch 33, Loss: 673.4978637695312, Accuracy: 0.5630780458450317\n",
      "Epoch 34, Loss: 674.5025634765625, Accuracy: 0.5630795955657959\n",
      "Epoch 35, Loss: 674.42236328125, Accuracy: 0.5630810856819153\n",
      "Epoch 36, Loss: 674.8535766601562, Accuracy: 0.5630825161933899\n",
      "Epoch 37, Loss: 674.4263916015625, Accuracy: 0.563083827495575\n",
      "Epoch 38, Loss: 674.4623413085938, Accuracy: 0.5630850791931152\n",
      "Epoch 39, Loss: 674.15087890625, Accuracy: 0.5630862712860107\n",
      "Epoch 40, Loss: 674.307861328125, Accuracy: 0.5630873441696167\n",
      "Epoch 41, Loss: 673.8662719726562, Accuracy: 0.5630884170532227\n",
      "Epoch 42, Loss: 673.7405395507812, Accuracy: 0.5630894899368286\n",
      "Epoch 43, Loss: 674.7935180664062, Accuracy: 0.563090443611145\n",
      "Epoch 44, Loss: 675.1763305664062, Accuracy: 0.5630913972854614\n",
      "Epoch 45, Loss: 674.259033203125, Accuracy: 0.5630922317504883\n",
      "Epoch 46, Loss: 674.7474975585938, Accuracy: 0.5630931258201599\n",
      "Epoch 47, Loss: 675.2619018554688, Accuracy: 0.563093900680542\n",
      "Epoch 48, Loss: 674.8502807617188, Accuracy: 0.5630946755409241\n",
      "Epoch 49, Loss: 674.1232299804688, Accuracy: 0.5630954504013062\n",
      "Epoch 50, Loss: 673.6154174804688, Accuracy: 0.5630961656570435\n"
     ]
    }
   ],
   "source": [
    "net = Net(input_size=num_features)\n",
    "criterion = nn.BCELoss(reduction='sum') # for binary prediction. Using the 'target' column\n",
    "#criterion = nn.MSELoss() # for regression, predicting the 'close' column\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(), lr=learning_rate\n",
    ")\n",
    "acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for seqs, labels in train_dataloader:\n",
    "        seqs = seqs.view(batch_size, sequence_len, num_features)\n",
    "        outputs = net(seqs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc(torch.round(outputs), labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {acc.compute()}\")\n",
    "    # early break if loss isn't changing beyond the learning rate\n",
    "    if loss.item() < .1:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:25:00.613475200Z",
     "start_time": "2024-07-31T15:23:05.265433400Z"
    }
   },
   "id": "2790534f5428cde2",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Define MSE metric\n",
    "# mse = torchmetrics.regression.MeanSquaredError()\n",
    "# \n",
    "# net.eval()\n",
    "# with torch.no_grad():\n",
    "#     for seqs, labels in test_dataloader:\n",
    "#         seqs = seqs.view(batch_size, torch_X_test.shape[1], num_features)\n",
    "#         # Pass seqs to net and squeeze the result\n",
    "#         outputs = net(seqs).squeeze()\n",
    "#         mse(outputs, labels)\n",
    "# \n",
    "# # Compute final metric value\n",
    "# test_mse = mse.compute()\n",
    "# print(f\"Test MSE: {test_mse}\")\n",
    "# print(f\"Test RMSE: {test_mse**.5}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:25:00.613475200Z",
     "start_time": "2024-07-31T15:25:00.608873200Z"
    }
   },
   "id": "ae6937e7eff7177e",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score: 0.5620105266571045\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in test_dataloader:\n",
    "        seqs = seqs.view(batch_size, sequence_len, num_features)\n",
    "        outputs = net(seqs).squeeze()\n",
    "        acc(torch.round(outputs), labels)\n",
    "\n",
    "print(f\"Test accuracy score: {acc.compute()}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:25:01.670608100Z",
     "start_time": "2024-07-31T15:25:00.611474800Z"
    }
   },
   "id": "a9824920abf3b48d",
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n",
    "Baseline guessing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bd50b5d6e213ee0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random guess likelihood 0.4687579112426077\n"
     ]
    }
   ],
   "source": [
    "random_guess_likelihood = df.target.mean()\n",
    "# mean = percent the candle was higher than the previous candle\n",
    "\n",
    "print('Random guess likelihood', random_guess_likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T15:25:01.671608Z",
     "start_time": "2024-07-31T15:25:01.666459500Z"
    }
   },
   "id": "acdd08bdbd74c444",
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Writeup\n",
    "At the minute level, 46.875% of the time the next close price is higher than the previous close price for VOO.\n",
    "The model is able to correctly predict a candle 56.201% of the time making it better than a random (weighted) coinflip. The thing holding the model back is that it doesn't take into account expected payoff. The average positive candel is higher than the average negative candle in absolute terms. The model is doing almost exactly as well on the train and test set so there is not any overfitting going on. The accuracy is a bit low though so even though it's a nn it has high bias and underfitting overall. It is just a hair less accurate than the LSTM.\n",
    "\n",
    "\n",
    "I used the following hyperparameters:\n",
    "sequence_len = 10\n",
    "* Sequence of length 10 seemed to give sufficient information without overwhelming my computer. Each input becomes sequence_len * feature so the compute cost is quite high increasing this value\n",
    "num_rows = 100000\n",
    "* This is to limit how much data I'm using. Adding more data doesn't seem to improve accuracy but it does crash performance.\n",
    "batch_size = 1000\n",
    "* Mostly arbitarily chosen. 1000 makes the model run faster and doesn't seem to sacrifice accuracy either.\n",
    "shuffle = False\n",
    "* don't want to shuffle sequence data since that defeats the purpose\n",
    "hidden_size = round(num_features / 2)\n",
    "* Heuristic that the best hidden_size is based on the average between the input and output sizes. Here my output is just 1 so it's just the number of features divided by 2\n",
    "num_layers = 2\n",
    "* Again heuristic, 2 hidden layers should be able to solve anything\n",
    "dropout = .5\n",
    "* Another mostly random chosen value. Playing around with this doesn't impact the model too much.\n",
    "\n",
    "learning_rate = 0.0001\n",
    "* small enough to change directions but not too small to where it doesn't update\n",
    "num_epochs = 50\n",
    "* Probably don't need this many. It stops really changing much after like 3-4 epochs\n",
    "\n",
    "\n",
    "I chose not to do dimensionality reduction for the GRU model. I'm allowing the model to figure out which features and relationships are important or not. A big part of the value of deep learning is that it doesn't require me doing any sort of feature engineering or reduction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3db37e546b6057c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
